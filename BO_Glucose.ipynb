{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from bayes_opt.blackbox_function import BlackBoxObjective\n",
    "from utils import extract_patient_meal_times\n",
    "\n",
    "torch.manual_seed(29)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "PATIENTS = pd.read_csv('subject_profiles.csv')\n",
    "NUM_MEALS = 3\n",
    "patients_3 = [idx for idx in range(40) if len(extract_patient_meal_times(PATIENTS.iloc[idx])) == NUM_MEALS]\n",
    "TASKS = patients_3\n",
    "TRAIN_TASKS = TASKS[:10] #TASKS[:len(patients_3) - 1]\n",
    "TEST_TASK = TASKS[len(patients_3) - 1]\n",
    "INPUT_DIM = NUM_MEALS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUNDS = torch.cat([torch.zeros((1, INPUT_DIM), dtype=dtype),\n",
    "                      torch.ones((1, INPUT_DIM), dtype=dtype)], dim=0)\n",
    "obj = BlackBoxObjective(num_meals=NUM_MEALS)\n",
    "def f(X, shift=TASKS[0], include_BMs=False):\n",
    "    \"\"\"\n",
    "    Torch-compatible objective function for the target_task\n",
    "    \"\"\"\n",
    "    if include_BMs: \n",
    "        f_X, BMs = obj.f(X=X, shift=shift, include_BMs=include_BMs)\n",
    "        return f_X, BMs\n",
    "    f_X = obj.f(X=X, shift=shift, include_BMs=include_BMs)\n",
    "    return f_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "noise_std = 0.001\n",
    "\n",
    "# Sample data for each base task\n",
    "data_by_task = {}\n",
    "for task in TRAIN_TASKS:\n",
    "    num_training_points = 20\n",
    "    # draw points from a sobol sequence\n",
    "    raw_x = draw_sobol_samples(bounds=BOUNDS, n=num_training_points, q=1, seed=task+5397923).squeeze(1)\n",
    "    # get observed values\n",
    "    f_x = f(raw_x, task)\n",
    "    train_y = f_x + noise_std*torch.randn_like(f_x)\n",
    "    train_yvar = torch.full_like(train_y, noise_std**2)\n",
    "    # store training data\n",
    "    data_by_task[task] = {\n",
    "        # scale x to [0, 1]\n",
    "        'train_x': normalize(raw_x, bounds=BOUNDS),\n",
    "        'train_y': train_y,\n",
    "        'train_yvar': train_yvar,\n",
    "    }         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models import FixedNoiseGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "\n",
    "\n",
    "def get_fitted_model(train_X, train_Y, train_Yvar, state_dict=None):\n",
    "    \"\"\"\n",
    "    Get a single task GP. The model will be fit unless a state_dict with model \n",
    "        hyperparameters is provided.\n",
    "    \"\"\"\n",
    "    Y_mean = train_Y.mean(dim=-2, keepdim=True)\n",
    "    Y_std = train_Y.std(dim=-2, keepdim=True)\n",
    "    model = FixedNoiseGP(train_X, (train_Y - Y_mean)/Y_std, train_Yvar)\n",
    "    model.Y_mean = Y_mean\n",
    "    model.Y_std = Y_std\n",
    "    if state_dict is None:\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model).to(train_X)\n",
    "        fit_gpytorch_model(mll)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting base model 0\n",
      "Fitting base model 1\n",
      "Fitting base model 2\n",
      "Fitting base model 3\n",
      "Fitting base model 4\n",
      "Fitting base model 5\n",
      "Fitting base model 7\n",
      "Fitting base model 8\n",
      "Fitting base model 10\n",
      "Fitting base model 11\n"
     ]
    }
   ],
   "source": [
    "# Fit base model\n",
    "base_model_list = []\n",
    "for task in TRAIN_TASKS:\n",
    "    print(f\"Fitting base model {task}\")\n",
    "    model = get_fitted_model(\n",
    "        data_by_task[task]['train_x'], \n",
    "        data_by_task[task]['train_y'], \n",
    "        data_by_task[task]['train_yvar'],\n",
    "    )\n",
    "    base_model_list.append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_col(X, shift):  \n",
    "    \"\"\"\n",
    "    Rotate columns to right by shift.\n",
    "    \"\"\"\n",
    "    return torch.cat((X[..., -shift:], X[..., :-shift]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_loss(f_samps, target_y):\n",
    "    \"\"\"\n",
    "    Compute ranking loss for each sample from the posterior over target points.\n",
    "    \n",
    "    Args:\n",
    "        f_samps: `n_samples x (n) x n`-dim tensor of samples\n",
    "        target_y: `n x 1`-dim tensor of targets\n",
    "    Returns:\n",
    "        Tensor: `n_samples`-dim tensor containing the ranking loss across each sample\n",
    "    \"\"\"\n",
    "    n = target_y.shape[0]\n",
    "    if f_samps.ndim == 3:\n",
    "        # Compute ranking loss for target model\n",
    "        # take cartesian product of target_y\n",
    "        cartesian_y = torch.cartesian_prod(\n",
    "            target_y.squeeze(-1), \n",
    "            target_y.squeeze(-1),\n",
    "        ).view(n, n, 2)\n",
    "        # the diagonal of f_samps are the out-of-sample predictions\n",
    "        # for each LOO model, compare the out of sample predictions to each in-sample prediction\n",
    "        rank_loss = ((f_samps.diagonal(dim1=1, dim2=2).unsqueeze(-1) < f_samps) ^ (cartesian_y[..., 0] < cartesian_y[..., 1])).sum(dim=-1).sum(dim=-1)\n",
    "    else:\n",
    "        rank_loss = torch.zeros(f_samps.shape[0], dtype=torch.long, device=target_y.device)\n",
    "        y_stack = target_y.squeeze(-1).expand(f_samps.shape)\n",
    "        for i in range(1,target_y.shape[0]):\n",
    "            rank_loss += ((roll_col(f_samps, i) < f_samps) ^ (roll_col(y_stack, i) < y_stack)).sum(dim=-1) \n",
    "    return rank_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_model_loocv_sample_preds(train_x, train_y, train_yvar, target_model, num_samples):\n",
    "    \"\"\"\n",
    "    Create a batch-mode LOOCV GP and draw a joint sample across all points from the target task.\n",
    "    \n",
    "    Args:\n",
    "        train_x: `n x d` tensor of training points\n",
    "        train_y: `n x 1` tensor of training targets\n",
    "        target_model: fitted target model\n",
    "        num_samples: number of mc samples to draw\n",
    "    \n",
    "    Return: `num_samples x n x n`-dim tensor of samples, where dim=1 represents the `n` LOO models,\n",
    "        and dim=2 represents the `n` training points.\n",
    "    \"\"\"\n",
    "    batch_size = len(train_x)\n",
    "    masks = torch.eye(len(train_x), dtype=torch.uint8, device=device).bool()\n",
    "    train_x_cv = torch.stack([train_x[~m] for m in masks])\n",
    "    train_y_cv = torch.stack([train_y[~m] for m in masks])\n",
    "    train_yvar_cv = torch.stack([train_yvar[~m] for m in masks])\n",
    "    state_dict = target_model.state_dict()\n",
    "    # expand to batch size of batch_mode LOOCV model\n",
    "    state_dict_expanded = {name: t.expand(batch_size, *[-1 for _ in range(t.ndim)]) for name, t in state_dict.items()}\n",
    "    model = get_fitted_model(train_x_cv, train_y_cv, train_yvar_cv, state_dict=state_dict_expanded)\n",
    "    with torch.no_grad():\n",
    "        posterior = model.posterior(train_x)\n",
    "        # Since we have a batch mode gp and model.posterior always returns an output dimension,\n",
    "        # the output from `posterior.sample()` here `num_samples x n x n x 1`, so let's squeeze\n",
    "        # the last dimension.\n",
    "        sampler = SobolQMCNormalSampler(num_samples=num_samples)\n",
    "        return sampler(posterior).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank_weights(train_x,train_y, base_models, target_model, num_samples):\n",
    "    \"\"\"\n",
    "    Compute ranking weights for each base model and the target model (using \n",
    "        LOOCV for the target model). Note: This implementation does not currently \n",
    "        address weight dilution, since we only have a small number of base models.\n",
    "    \n",
    "    Args:\n",
    "        train_x: `n x d` tensor of training points (for target task)\n",
    "        train_y: `n` tensor of training targets (for target task)\n",
    "        base_models: list of base models\n",
    "        target_model: target model\n",
    "        num_samples: number of mc samples\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: `n_t`-dim tensor with the ranking weight for each model\n",
    "    \"\"\"\n",
    "    ranking_losses = []\n",
    "    # compute ranking loss for each base model\n",
    "    for task in range(len(base_models)):\n",
    "        model = base_models[task]\n",
    "        # compute posterior over training points for target task\n",
    "        posterior = model.posterior(train_x)\n",
    "        sampler = SobolQMCNormalSampler(num_samples=num_samples)\n",
    "        base_f_samps = sampler(posterior).squeeze(-1).squeeze(-1)\n",
    "        # compute and save ranking loss\n",
    "        ranking_losses.append(compute_ranking_loss(base_f_samps, train_y))\n",
    "    # compute ranking loss for target model using LOOCV\n",
    "    # f_samps\n",
    "    target_f_samps = get_target_model_loocv_sample_preds(train_x, train_y, train_yvar, target_model, num_samples)\n",
    "    ranking_losses.append(compute_ranking_loss(target_f_samps, train_y))\n",
    "    ranking_loss_tensor = torch.stack(ranking_losses)\n",
    "    # compute best model (minimum ranking loss) for each sample\n",
    "    best_models = torch.argmin(ranking_loss_tensor, dim=0)\n",
    "    # compute proportion of samples for which each model is best\n",
    "    rank_weights = best_models.bincount(minlength=len(ranking_losses)).type_as(train_x)/num_samples\n",
    "    return rank_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from gpytorch.models import GP\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.lazy import PsdSumLazyTensor\n",
    "from gpytorch.likelihoods import LikelihoodList\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "\n",
    "class RGPE(GP, GPyTorchModel):\n",
    "    \"\"\"\n",
    "    Rank-weighted GP ensemble. Note: this class inherits from GPyTorchModel which provides an \n",
    "        interface for GPyTorch models in botorch.\n",
    "    \"\"\"\n",
    "    \n",
    "    _num_outputs = 1  # metadata for botorch\n",
    "    \n",
    "    def __init__(self, models, weights):\n",
    "        super().__init__()\n",
    "        self.models = ModuleList(models)\n",
    "        for m in models:\n",
    "            if not hasattr(m, \"likelihood\"):\n",
    "                raise ValueError(\n",
    "                    \"RGPE currently only supports models that have a likelihood (e.g. ExactGPs)\"\n",
    "                )\n",
    "        self.likelihood = LikelihoodList(*[m.likelihood for m in models])\n",
    "        self.weights = weights\n",
    "        self.to(weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weighted_means = []\n",
    "        weighted_covars = []\n",
    "        # filter model with zero weights\n",
    "        # weights on covariance matrices are weight**2\n",
    "        non_zero_weight_indices = (self.weights**2 > 0).nonzero()\n",
    "        non_zero_weights = self.weights[non_zero_weight_indices]\n",
    "        # re-normalize\n",
    "        non_zero_weights /= non_zero_weights.sum()\n",
    "        \n",
    "        for non_zero_weight_idx in range(non_zero_weight_indices.shape[0]):\n",
    "            raw_idx = non_zero_weight_indices[non_zero_weight_idx].item()\n",
    "            model = self.models[raw_idx]\n",
    "            posterior = model.posterior(x)\n",
    "            # unstandardize predictions\n",
    "            posterior_mean = posterior.mean.squeeze(-1)*model.Y_std + model.Y_mean\n",
    "            posterior_cov = posterior.mvn.lazy_covariance_matrix * model.Y_std.pow(2)\n",
    "            # apply weight\n",
    "            weight = non_zero_weights[non_zero_weight_idx]\n",
    "            weighted_means.append(weight * posterior_mean)\n",
    "            weighted_covars.append(posterior_cov * weight**2)\n",
    "        # set mean and covariance to be the rank-weighted sum the means and covariances of the\n",
    "        # base models and target model\n",
    "        mean_x = torch.stack(weighted_means).sum(dim=0)\n",
    "        covar_x = PsdSumLazyTensor(*weighted_covars)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 of 1\n",
      "10 [array([[140.        , 251.        , 400.        , 725.        ],\n",
      "       [  5.73768157,   4.86401641,   3.38410573,   3.47437154]])] -16.77032906320135\n",
      "11 [array([[140.        , 251.        , 400.        , 725.        ],\n",
      "       [  5.67599614,   5.52736752,   4.62616401,   2.38960295]])] -15.104776553363617\n",
      "11 [array([[140.        , 251.        , 400.        , 725.        ],\n",
      "       [  5.67599614,   5.52736752,   4.62616401,   2.38960295]])] -15.104776553363617\n",
      "13 [array([[140.        , 251.        , 400.        , 725.        ],\n",
      "       [  5.71239497,   6.12798231,   5.04671762,   3.68597619]])] -13.02074787955188\n",
      "13 [array([[140.        , 251.        , 400.        , 725.        ],\n",
      "       [  5.71239497,   6.12798231,   5.04671762,   3.68597619]])] -13.02074787955188\n"
     ]
    }
   ],
   "source": [
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "# suppress GPyTorch warnings about adding jitter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"^.*jitter.*\", category=RuntimeWarning)\n",
    "\n",
    "    \n",
    "best_rgpe_all = []\n",
    "best_argmax_rgpe_all = []\n",
    "best_random_all = []\n",
    "best_vanilla_nei_all = []\n",
    "N_BATCH = 5\n",
    "NUM_POSTERIOR_SAMPLES = 256\n",
    "RANDOM_INITIALIZATION_SIZE = 10\n",
    "N_TRIALS = 1\n",
    "MC_SAMPLES = 512\n",
    "N_RESTART_CANDIDATES = 512\n",
    "N_RESTARTS = 10\n",
    "Q_BATCH_SIZE = 1\n",
    "\n",
    "# Average over multiple trials\n",
    "for trial in range(N_TRIALS):\n",
    "    print(f\"Trial {trial + 1} of {N_TRIALS}\")\n",
    "    best_BMs = []\n",
    "    best_rgpe = []\n",
    "    best_random = [] \n",
    "    best_vanilla_nei = []\n",
    "    # Initial random observations\n",
    "    raw_x = draw_sobol_samples(bounds=BOUNDS, n=RANDOM_INITIALIZATION_SIZE, q=1, seed=trial).squeeze(1)    \n",
    "    train_x = normalize(raw_x, bounds=BOUNDS)\n",
    "    train_y_noiseless = f(raw_x, shift=TEST_TASK) \n",
    "    train_y = train_y_noiseless + noise_std*torch.randn_like(train_y_noiseless)\n",
    "    train_yvar = torch.full_like(train_y, noise_std**2)\n",
    "    # keep track of the best observed point at each iteration\n",
    "    best_value = train_y.max().item()\n",
    "    best_rgpe.append(best_value)\n",
    "    best_random.append(best_value)\n",
    "    vanilla_nei_best_value = best_value\n",
    "    best_vanilla_nei.append(vanilla_nei_best_value)\n",
    "\n",
    "    # Run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "    for iteration in range(N_BATCH): \n",
    "        target_model = get_fitted_model(train_x, train_y, train_yvar)\n",
    "        model_list = base_model_list + [target_model]\n",
    "        rank_weights = compute_rank_weights(\n",
    "            train_x, \n",
    "            train_y, \n",
    "            base_model_list, \n",
    "            target_model, \n",
    "            NUM_POSTERIOR_SAMPLES,\n",
    "        )\n",
    "       \n",
    "        # create model and acquisition function\n",
    "        rgpe_model = RGPE(model_list, rank_weights)\n",
    "        sampler_qnei = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        qNEI = qNoisyExpectedImprovement(\n",
    "            model=rgpe_model, \n",
    "            X_baseline=train_x,\n",
    "            sampler=sampler_qnei,\n",
    "        )\n",
    "        \n",
    "        # optimize\n",
    "        candidate, _ = optimize_acqf(\n",
    "            acq_function=qNEI,\n",
    "            bounds=BOUNDS,\n",
    "            q=Q_BATCH_SIZE,\n",
    "            num_restarts=N_RESTARTS,\n",
    "            raw_samples=N_RESTART_CANDIDATES,\n",
    "        )\n",
    "\n",
    "        # fetch the new values \n",
    "        new_x = candidate.detach()\n",
    "        new_y_noiseless = f(unnormalize(new_x, bounds=BOUNDS), shift=TEST_TASK)\n",
    "        new_y = new_y_noiseless + noise_std*torch.randn_like(new_y_noiseless)\n",
    "        new_yvar = torch.full_like(new_y, noise_std**2)\n",
    "\n",
    "        # update training points\n",
    "        train_x = torch.cat((train_x, new_x))\n",
    "        train_y = torch.cat((train_y, new_y))\n",
    "        train_yvar = torch.cat((train_yvar, new_yvar))\n",
    "        random_candidate = torch.rand((1, NUM_MEALS + 1), dtype=dtype, device=device)\n",
    "        next_random_noiseless = f(unnormalize(random_candidate, bounds=BOUNDS), shift=TEST_TASK)\n",
    "        next_random = next_random_noiseless + noise_std * torch.randn_like(next_random_noiseless)\n",
    "        next_random_best = next_random.max().item()\n",
    "        best_random.append(max(best_random[-1], next_random_best))\n",
    "\n",
    "        # get the new best observed value\n",
    "        best_value = train_y.max().item()\n",
    "        best_idx = torch.argmax(train_y).item()\n",
    "        best_candidate = train_x[best_idx].view(1, -1)\n",
    "        _, best_BM = f(unnormalize(best_candidate, bounds=BOUNDS),\n",
    "                         shift=TEST_TASK,\n",
    "                         include_BMs=True)\n",
    "        \n",
    "        print(best_idx, best_BM, best_value)\n",
    "        best_rgpe.append(best_value)\n",
    "        best_BMs.append(best_BM)\n",
    "        \n",
    "    best_rgpe_all.append(best_rgpe)\n",
    "    best_argmax_rgpe_all.append(best_BMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_rgpe_all = np.array(best_rgpe_all)\n",
    "best_random_all = np.array(best_random_all)\n",
    "best_vanilla_nei_all = np.array(best_vanilla_nei_all)\n",
    "\n",
    "x = range(RANDOM_INITIALIZATION_SIZE, RANDOM_INITIALIZATION_SIZE + N_BATCH + 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "# Plot RGPE - NEI\n",
    "ax.errorbar(\n",
    "    x, \n",
    "    best_rgpe_all.mean(axis=0), \n",
    "    yerr=1.96 * best_rgpe_all.std(axis=0) / math.sqrt(N_TRIALS), \n",
    "    label=\"RGPE - NEI\", \n",
    "    linewidth=3, \n",
    "    capsize=5,\n",
    "    capthick=3,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Best Observed Value', fontsize=12)\n",
    "ax.set_title('Best Observed Value by Iteration', fontsize=12)\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GlucoseControl_env",
   "language": "python",
   "name": "glucosecontrol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
